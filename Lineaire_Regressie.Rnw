\documentclass{tufte-book}
\usepackage{graphicx}  % werken met figuren
\usepackage{gensymb} % werken met wetenschappelijke eenheden\usepackage{geometry}
\usepackage{changepage} % http://ctan.org/pkg/changepage
\usepackage[dutch,british]{babel} % instelling van de taal (woordsplitsing, spellingscontrole)
\usepackage[parfill]{parskip} % Paragrafen gescheiden door witte lijn en geen inspringing
\usepackage[font=small,skip=3pt]{caption} % Minder ruimte tussen figuur/table en ondertitel. Ondertitel klein
\usepackage{capt-of}
\usepackage{indentfirst}
\setlength{\parindent}{0.7cm}
\usepackage{enumitem} % Laat enumerate werken met letters
\usepackage{url}
\usepackage{lipsum}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
% Prints a trailing space in a smart way.
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{amsmath}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% Alter some LaTeX defaults for better treatment of figures:
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.9}	% max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \renewcommand{\textfraction}{0.1}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.8}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\setcounter{secnumdepth}{3}

\newcommand{\tthdump}[1]{#1}

\newcommand{\openepigraph}[2]{
  \begin{fullwidth}
  \sffamily\large
    \begin{doublespace}
      \noindent\allcaps{#1}\\ % epigraph
      \noindent\allcaps{#2} % author
    \end{doublespace}
  \end{fullwidth}
}


\usepackage{makeidx}
\makeindex

\title{Enkelvoudige Lineair Regressie}
\author{Jan Trommelmans}

\begin{document}
\SweaveOpts{concordance=TRUE,prefix.string=LinReg}
\setkeys{Gin}{width=1.1\marginparwidth} %% Sweave

<<echo=FALSE>>=
library(tidyverse)
library(broom)
library(gridExtra)
library(nortest)
library(car)
@

% Setting the ggplot theme:
<<echo=FALSE>>=
JT.theme <- theme(panel.border = element_rect(fill = NA, colour = "gray10"),
                  panel.background = element_blank(),
                  panel.grid.major = element_line(colour = "gray85"),
                  panel.grid.minor = element_line(colour = "gray85"),
                  panel.grid.major.x = element_line(colour = "gray85"),
                  axis.text = element_text(size = 9 , face = "bold"),
                  axis.title = element_text(size = 10 , face = "bold"),
                  plot.title = element_text(size = 12 , face = "bold"),
                  strip.text = element_text(size = 9 , face = "bold"),
                  strip.background = element_rect(colour = "black"),
                  legend.text = element_text(size = 9),
                  legend.title = element_text(size = 10 , face = "bold"),
                  legend.background = element_rect(fill = "white"),
                  legend.key = element_rect(fill = "white"))
@

% Functions

\mainmatter

\chapter{Enkelvoudige Lineaire Regressie}

\section{Inleiding}

Het onderscheid tussen een \emph{functioneel} en een \emph{statistisch} verband is dat de waarnemingen bij een statistisch verband \emph{niet} precies gelegen zijn op een lijn beschreven door een functie $y=f(x)$, maar in de buurt daarvan (Figuur~\ref{fig:intro1}).

<<label=intro1,fig=TRUE,include=FALSE, echo=FALSE>>=
N <- 2000
b0 <- -1
b1 <- 7
sigma_U <- 2
set.seed(2018)
intro1 <- data.frame(x = runif(N, 1, 5), y=0)
intro1$y <- b0 + b1*intro1$x + rnorm(N, 0, sigma_U)
ggplot(data=intro1) +
  geom_point(aes(x = x, y = y), alpha=0.2) +
  geom_abline(intercept=-1, slope=7, color="blue") +
  xlim(0, 6) +
  labs(title = "Een statistisch verband y=f(x)", 
       x = "onafhankelijke variabele x", 
       y = "responsvariabele y") +
  JT.theme
@

\begin{marginfigure}[0cm]
\includegraphics[width=1\textwidth]{LinReg-intro1}
\caption{ }
\label{fig:intro1}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

De oorzaak is dat de responsvariabele $y$ niet enkel be\"invloed wordt door $n$ onafhankelijke variabelen $x_{i}; i=1 \ldots n$ maar ook door $k$ onbekende en/of oncontroleerbare storende variabelen $z_{j}; j=1 \ldots k$. We kunnen dit schrijven als:

\begin{equation}
y=f(x_{1}, x_{2}, \ldots x_{n}; z_{1}, z_{2} \ldots z_{k})
\end{equation}

Het gevolg hiervan is dat bij herhaalde experimenten met eenzelfde instelling van de onafhankelijke variabelen $x_{i}$, we andere waarden voor de responsvariabele $y$ zullen krijgen. Figuur~\ref{fig:intro2} geeft de situatie waarbij we maar \'e\'en onafhankelijke variabele $x$ hebben die op de waarden 1, 2, 3, 4 en 5 wordt ingesteld bij 10 herhalingsmetingen.

<<label=intro2,fig=TRUE,include=FALSE, echo=FALSE>>=
N <- 10
n <- 5
b0 <- -1
b1 <- 7
sigma_U <- 2
set.seed(2018)
intro2 <- data.frame(expnr = rep((1:N), each=n), x = rep(seq(1,n),N), y = 0)
intro2$y <- b0 + b1*intro2$x + rnorm(n*N, 0, sigma_U)
ggplot(data=intro2) +
  geom_point(aes(x = x, y = y, color=factor(expnr))) +
  xlim(0, 6) +
  labs(title = "Experimentele resultaten", 
       subtitle = "10 herhalingsexperimenten",
       x = "onafhankelijke variabele x", 
       y = "waarneming van responsvariabele y") +
  JT.theme
@

\begin{marginfigure}[-2cm]
\includegraphics[width=1\textwidth]{LinReg-intro2}
\caption{ }
\label{fig:intro2}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

De waarde van de responsvariabele $y_{i}$ is omwille van de storende factoren $z_{j}$ een \emph{kansvariabele} $Y_{i}$. De verstoring vormt het kansgedeelte dat we $U_{i}$ noemen en waarvan we aannemen\sidenote{Veronderstelling 1: $E(U_{i})=0$} dat de gemiddelde waarde $E(U_{i})=0$. We nemen aan dat de gemiddelde waarden van $Y_{i}$ beschreven wordt door een functie van de onafhankelijke variabelen $x_{i}$ en dat deze functie niet enkel geldt voor de waargenomen waarden van $x_{i}$ maar voor alle $x$:
\begin{equation}
  \begin{split}
  Y_{i}=E(Y_{i}|x_{1}, x_{2}, \ldots x_{n}) + U_{i}&=f(x_{1}, x_{2}, \ldots x_{n}) + U_{i} \\
  E(Y_{i}|x_{1}, x_{2}, \ldots x_{n})&=f(x_{1}, x_{2}, \ldots x_{n})  + 0 \\
  & en \\
  Y=E(Y|x_{1}, x_{2}, \ldots x_{n}) + U&=f(x_{1}, x_{2}, \ldots x_{n}) + U \\
  E(Y|x_{1}, x_{2}, \ldots x_{n})&=f(x_{1}, x_{2}, \ldots x_{n})  + 0
  \end{split}
\end{equation}

\section{Enkelvoudige lineair regressie}
\newthought{Bij een enkelvoudige relatie} is er slechts \'e\'en onafhankelijke variabele $x$ en worden de vergelijkingen:
\begin{equation}
  \begin{split}
  Y_{i}=E(Y_{i}|x_{i}) + U_{i}&=f(x_{i}) + U_{i} \\
  E(Y_{i}|x_{i})&=f(x_{i})  + 0 \\
  & en \\
  Y=E(Y|x) + U&=f(x) + U \\
  E(Y|x)&=f(x)  + 0 
  \end{split}
\end{equation}

\subsection{Lineair verband}
\newthought{Een (eenmalig) experiment} geeft volgende resultaten (Figuur~\ref{fig:geg}):
<<label=geg,fig=TRUE,include=FALSE, echo=FALSE>>=
geg <- data.frame(x = seq(1,5), y = c(1, 1, 2, 2, 4))
n <- nrow(geg)
ggplot(data=geg) +
  geom_point(aes(x = x, y = y)) +
  xlim(0, 6) +
  ylim(0, 6) +
  labs(title = "Startgegevens", x = "onafhankelijke variabele x", y = "responsvariabele y") +
  JT.theme
@

\begin{marginfigure}[3cm]
\includegraphics[width=1\textwidth]{LinReg-geg}
\caption{ }
\label{fig:geg}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

<<>>=
geg
@

We veronderstellen\sidenote{Veronderstelling 2: de vorm van de functie f(x) is $E(Y)=\beta_{0} + \beta_{1}x$} dat de achterliggende Platonische waarheid een lineair verband tussen $E(Y)$ en de onafhankelijke variabele $x$ is:
\begin{equation}
E(Y)=\beta_{0} + \beta_{1}x
\end{equation}

De waarneming $Y$ is een kansveranderlijke veroorzaakt door de storende factoren waarvan de effecten gegroepeerd zijn in de kansveranderlijke U:
\begin{equation}
Y=E(Y)+U=\beta_{0} + \beta_{1}x + U
\end{equation}

\newpage
\subsection{Schatting van $\beta_{0}$ en $\beta_{1}$ met de kleinste kwadraten methode}
De methode van de kleinste kwadraten wordt gebruikt om uit \emph{deze} steekproefwaarden een schatting te halen voor $\beta_{0}$ en $\beta_{1}$. De methodiek is bekend: men minimaliseert het criterium:
\begin{equation}
Q=\sum\limits_{i=1}^{i=n}\left( Y_{i}-\beta_{0}-\beta_{1}x_{i}  \right)^{2}
\end{equation}

Daarvoor berekenen we de parti\"ele afgeleiden van $Q$ naar $\beta_{0}$ en $\beta_{1}$ en stellen die gelijk aan nul. Uit de twee daaruit voortvloeiende \emph{normaalvergelijkingen} volgen twee getalwaarden $b_{0}$ en $b_{1}$ die bere-kend worden op basis van de steekproefgegevens. Hierbij worden wat tussenresultaten berekend. 
\begin{equation}
  \begin{split}
  E(X)&=\bar{x}=\sum_{i=1}^{i=n}x_{i}\frac{1}{n} \\
  var(X)&=\frac{\sum_{i=1}^{i=n}(x_{i} - E(X))^2}{n-1}=\frac{\sum_{i=1}^{i=n}(x_{i} - \bar{x})^2}{n-1}=\frac{SS_{xx}}{n-1} \\
  E(Y)&=\bar{y}=\sum_{i=1}^{i=n}y_{i}\frac{1}{n} \\
  var(Y)&=\frac{\sum_{i=1}^{i=n}(y_{i} - E(Y))^2}{n-1}=\frac{\sum_{i=1}^{i=n}(y_{i} - \bar{y})^2}{n-1}=\frac{SS_{yy}}{n-1} \\
  covar(X,Y)&=\frac{\sum_{i=1}^{i=n}(x_{i}-\bar{x})(y_{i}-\bar{y})}{n-1}=\frac{S_{xy}}{n-1}
  \end{split}
\end{equation}

<<echo=FALSE>>=
gem_x <- sum(geg$x)/n
ss_xx <- sum((geg$x-gem_x)^2)
var_x <- ss_xx/(n-1)
gem_y <- sum(geg$y)/n
ss_yy <- sum((geg$y - gem_y)^2)
var_y <- ss_yy/(n-1)
s_xy <- sum((geg$x-gem_x)*(geg$y-gem_y))
covar_xy <- s_xy/(n-1)
b1 <- s_xy/ss_xx
b0 <- gem_y - b1*gem_x
geg$diffx <- geg$x - gem_x
geg$diffx2 <- (geg$x - gem_x)^2
geg$diffy <- geg$y - gem_y
geg$diffy2 <- (geg$y - gem_y)^2
@

\begin{center}
\begin{tabular}{ r l }
 $\bar{x}=$ & \Sexpr{gem_x}  \\
 $SS_{xx}=$ & \Sexpr{ss_xx}  \\
 $var(x)=$ & \Sexpr{var_x} \\
 $\bar{y}=$ & \Sexpr{gem_y} \\
 $SS_{yy}=$ & \Sexpr{ss_yy} \\
 $var(y)=$ & \Sexpr{var_y} \\
 $S_{xy}=$ & \Sexpr{s_xy} \\
 $covar(x,y)=$ & \Sexpr{covar_xy} \\
 $b_{1}=\frac{\sum\limits_{i=1}^{i=n} (x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum\limits_{i=1}^{i=n}(x_{i}-\bar{x})^{2}}=\frac{S_{xy}}{SS_{xx}}=$ & \Sexpr{b1} \\
 $b_{0}=\bar{y} - b_{1}\bar{x}=$ & \Sexpr{round(b0,1)}
\end{tabular}
\end{center}
\medskip
Met deze waarden van $b_{0}$ en $b_{1}$ kunnen de berekende waarden ($\hat{y}_{i}$) en het verschil tussen deze berekende en waargenomen waarde ($y_{i}$) bepaald worden. Dat verschil noemen we het \emph{residu}:
\begin{equation}
r_{i}=y_{i}-\hat{y}_{i}=y_{i}-b_{0}-b_{1}x_{i}
\end{equation}

Het residu $r_{i}$ is dus niet enkel afhankelijk van de waarneming (die verstoord wordt door $U_{i}$) maar ook door de waarden van de steekproef die $b_{0}$ en $b_{1}$ bepalen. De variantie van $r_{i}$ zal dus groter zijn dan de variantie van de foutterm $U_{i}$. We maken daarom een duidelijk onderscheid tussen de foutterm $U(x)$ (die niet toegankelijk is) en de residus $r_{i}$ (die, voor elke steekproef, berekenbaar zijn). Bovendien zal de variantie van $r_{i}$ \emph{verschillend} zijn van de variantie van $r_{j}$ wanneer $i \neq j$! Zie hiervoor \ref{subsec:Veronderstelling3}.

\begin{equation}
  \begin{split}
  \hat{y}_{i} &= b_{0}+b_{1}x_{i} \\
  r_{i}&= y_{i} - \hat{y}_{i}
  \end{split}
\end{equation}

<<echo=FALSE>>=
geg$yhat <- round(b0 + b1*geg$x, 2)
geg$r <- geg$y - geg$yhat
geg$rx <- geg$x*geg$r
geg$ryhat <- geg$yhat*geg$r
tabel <- geg
som <- round(colSums(geg),0)
tabel <- rbind(tabel,som)
@
(De onderste rij geeft de som van alle kolommen.)
<<>>=
tabel
@
\medskip
Wanneer we de parameters $b_{0}$ en $b_{1}$ bepalen met de kleinste kwadraten methode dan heeft de berekende rechte volgende eigenschappen:
\begin{itemize}
  \item de rechte gaat door het zwaartepunt $(\bar{x},\bar{y})$. Dit volgt uit de vergelijking voor $b_{0}$ die we kunnen herschrijven als $\bar{y}=b_{0}+b_{1}\bar{x}$.
  \item de som van de residus $r_{i}$ is nul. Dat volgt uit
  \begin{equation}
  \sum\limits_{i=1}^{i=n}r_{i}=\sum\limits_{i=1}^{i=n}(y_{i}-b_{0}-b_{1}x_{i})=n\bar{y}-nb_{0}-nb_{1}\bar{x}=n(\bar{y}-b_{0}-b_{1}\bar{x})=0
  \end{equation}
   \item de som van de berekende waarden $y_{i}$ is gelijk aan de som van de waargenomen waarden $y_{i}$: $\sum_{i=1}^{i=n}\hat{y}_{i}=\sum_{i=1}^{i=n}y_{i}$. Dat volgt uit het voorgaande.
  \item de residus $r_{i}$ en de onafhankelijke waarden $x_{i}$ zijn ortogonaal\sidenote{''Kwantitatieve beleidsmethoden: enkelvoudige en meervoudige regressie", Peter Goos, 2014, pp. 13}.
  \item de residus $r_{i}$ en de berekende waarden $\hat{y}_{i}$ zijn ortogonaal. Dat volgt uit het voorgaande en het lineair verband tussen $\hat{Y}$ en $x$.
\end{itemize}

\subsection{Eigenschappen van de kleinste kwadratenschatters}

\newthought{Na het uitvoeren van een nieuw experiment} krijg je andere waarden voor $y_{i}$ (niet voor $x_{i}$!). Daaruit volgt dat $SS_{xx}$ hetzelfde blijft, maar $SS_{yy}$ en $S_{xy}$ veranderen. Daardoor verandert $b_{1}=\frac{S_{xy}}{SS_{xx}}$ en dus ook $b_{0}=\bar{y}-b_{1}\bar{x}$. Het toeval dat de fouten U genereert, zal dus ook de kleinste kwadratenschatters be\"invloeden. Dit betekent dat $b_{0}$ en $b_{1}$ manifestaties zijn van kansvariabelen $B_{0}$ en $B_{1}$.

We kunnen bewijzen\sidenote{Goos pp. 14 e.v.} dat $b_{0}$ en $b_{1}$ onvertekende schatters zijn van de Platonische $\beta_{0}$ en $\beta_{1}$:
\begin{equation}
  \begin{split}
  E(B_{0})&=\beta_{0} \\
  E(B_{1})&=\beta_{1}
  \end{split}
\end{equation}

We kunnen ook de variantie (en dus de standaardafwijking) van $B_{0}$ en $B_{1}$ berekenen, mits wat bijkomende voorwaarden. De nieuwe veronderstellingen zijn:
\begin{itemize}
  \item \emph{homoscedasticiteit}: $var(U_{i})=\sigma^{2}=constant$\sidenote{Veronderstelling 3: de variantie van alle fouttermen is gelijk}
  \item \emph{onafhankelijkheid van de fouttermen}: $cov(U_{i},U_{j})=0$ voor $i \neq j$\sidenote{Veronderstelling 4: onafhankelijkheid van de fouttermen}
\end{itemize}

In dat geval is
\begin{equation}
  \begin{split}
  var(B_{0}) &= \frac{\sigma^{2}}{SS_{xx}}\left[ \frac{\sum\limits_{i=1}^{i=n}x_{i}^{2}}{n} \right] \\
  var(B_{1}) &= \frac{\sigma^{2}}{SS_{xx}} \\
  cov(B_{0}, B_{1}) &= -\frac{\sigma^{2}}{SS_{xx}}\bar{x}
  \end{split}
\end{equation}

We zien overal de breuk $\frac{\sigma^{2}}{SS_{xx}}$ opduiken, met in de teller de \emph{onbekende} variantie van de foutterm en in de noemer de \emph{constante} $SS_{xx}$.

De \emph{Gauss-Markov stelling} toont aan dat de schatters voor $\beta_{0}$ en $\beta_{1}$ die we bekomen via de kleinste kwadratenmethode niet alleen onvertekend zijn, maar bovendien de kleinste variantie hebben van alle onvertekende schatters die lineair zijn in de responsen $Y_{i}$.

\subsection{Een schatter voor $\sigma^{2}$}

De varianties van $B_{0}$ en $B_{1}$ zijn afhankelijk van de (constant veronderstelde) variantie van de foutterm. Aangezien $B_{0}$, $B_{1}$ en $Y_{i}$ kansvariabelen zijn, zal $R_{i}=Y_{i}-B_{0}-B_{1}x_{i}$ ook een kansveranderlijke zijn en dus ook de som van de kwadraten ervan (SSE= Sum of Squared Errors):
\begin{equation}
SSE=\sum\limits_{i=1}^{i=n}R_{i}^{2}=\sum\limits_{i=1}^{i=n}(Y_{i}-B_{0}-B_{1}x_{i})^{2}
\end{equation}

Het aantal vrijheidsgraden van SSE is (n-2) omdat twee vrijheidsgraden verloren gaan omdat we  $B_{0}$ en $B_{1}$ moeten schatten. Men kan aantonen dat
\begin{equation}
E(\frac{SSE}{n-2})=E(MSE)=\sigma^{2}
\end{equation}

Voor \'e\'en bepaald experiment krijgt de kansveranderlijke SSE een concrete waarde nl;
\begin{equation}
sse=\sum\limits_{i=1}^{i=n}r_{i}^{2}=\sum\limits_{i=1}^{i=n}(y_{i}-b_{0}-b_{1}x_{i})^{2}
\end{equation}

Een schatter voor de onbekende $\sigma^{2}$ is dus
\begin{equation}
s^{2}=\frac{sse}{n-2}=\frac{\sum\limits_{i=1}^{i=n}(y_{i}-b_{0}-b_{1}x_{i})^{2}}{n-2}=\frac{r_{i}^{2}}{n-2}
\end{equation}

<<echo=FALSE>>=
sse <- sum(geg$r^2)
s_sq <- sse/(n-2)
s <- sqrt(s_sq)
@

In dit voorbeeld is $s^{2}=$\Sexpr{round(s_sq,2)}. De schatters voor de variantie van $B_{0}$, $B_{1}$ en hun covariantie worden gegeven door:
\begin{equation}
  \begin{split}
  s^{2}_{B_{0}} &= \frac{s^{2}}{SS_{xx}}\left[ \frac{\sum\limits_{i=1}^{i=n}x_{i}^{2}}{n} \right] = \frac{sse^{2}}{(n-2)SS_{xx}}\left[\frac{\sum\limits_{i=1}^{i=n}x_{i}^{2}}{n}\right] \\
  s^{2}_{B_{1}} &= \frac{s^{2}}{SS_{xx}} =  \frac{sse^{2}}{(n-2)SS_{xx}}\\
  s_{B_{0}, B_{1}} &= -\frac{s^{2}}{SS_{xx}}\bar{x} = -\frac{sse^{2}}{(n-2)SS_{xx}}\bar{x}
  \end{split}
\end{equation}

<<echo=FALSE>>=
s_b0 <- (s/sqrt(ss_xx))*(sqrt(sum(geg$x^2))/n)
s_b1 <- (s/sqrt(ss_xx))
@

De schatting voor de standaardafwijking van $b_{0}$ is \Sexpr{round(s_b0,2)} en van $b_{1}$ is \Sexpr{round(s_b1,2)}.

\subsection{Betrouwbaarheidsintervallen voor $\beta_{0}$ en $\beta_{1}$}
Als we veronderstellen dat de foutterm $U(x)$ normaal verdeeld is $N(0,\sigma)$\sidenote{Veronderstelling 5: de kansvariabelen $U_{i}$ zijn normaal verdeeld}, dan volgt daaruit dat ook de kansvariabelen $Y_{i}$ en dus ook $Y$ normaal verdeeld zijn (de som van een constante $E(Y_{i})$ en een normaal verdeelde kansvariabele). Aangezien de kleinste kwadratenschatters $B_{0}$ en $B_{1}$ lineaire combinaties zijn van de waarnemingen $Y_{i}$ zullen die ook normaal verdeeld zijn. We kennen hun gemiddelde waarde en hun variantie en kunnen dus direct schrijven dat:
\begin{equation}
  \begin{split}
  B_{0} &\sim N(\beta_{0}, \frac{\sigma}{\sqrt{SS_{xx}}}\left[\sqrt{\frac{\sum\limits_{i=1}^{i=n}x_{i}^{2}}{n})}\right] \\
  B_{1} &\sim N(\beta_{1}, \frac{\sigma}{\sqrt{SS_{xx}}})
  \end{split}
\end{equation}

We kennen de juiste waarde van $\sigma$ niet, maar wel de schatter ervan $s$. Dan weten we dat:
\begin{equation}
  \begin{split}
  \frac{B_{0} - \beta_{0}}{\frac{s}{\sqrt{SS_{xx}}}\left[\sqrt{\frac{\sum\limits_{i=1}^{i=n}x_{i}^{2}}{n})}\right]}  &\sim T_{(n-2)}  \\
  \frac{B_{1} - \beta_{1}}{\frac{s}{\sqrt{SS_{xx}}}} &\sim T_{n-2}
  \end{split}
\end{equation}

Hiermee kunnen we intervallen bepalen voor $\beta_{0}$ en $\beta_{1}$ met een betrouwbaarheid $\alpha$:
\begin{equation}
  \begin{split}
  t_{0}&=F_{T}^{-1}(1-\frac{\alpha}{2}, n-2) \\
  b_{0} - t_{0}\frac{s}{\sqrt{SS_{xx}}}\left[\sqrt{\frac{\sum\limits_{i=1}^{i=n}x_{i}^{2}}{n})}\right] &< \beta_{0} < b_{0} + t_{0}\frac{s}{\sqrt{SS_{xx}}}\left[\sqrt{\frac{\sum\limits_{i=1}^{i=n}x_{i}^{2}}{n})}\right] \\
  b_{1}-t_{0}\frac{s}{\sqrt{SS_{xx}}} &< \beta_{1} <  b_{1}+t_{0}\frac{s}{\sqrt{SS_{xx}}}
  \end{split}
\end{equation}

In R de inverse cumulatieve kansfunctie is $qt(x,df)$ zodat $t_{0}=qt(1-\frac{\alpha}{2}, n-2)$. In dit voorbeeld zijn de 95\%-betrouwbaarheidsintervallen:
<<echo=FALSE>>=
betrouwbaarheid <- 0.95
alpha <- 1 - betrouwbaarheid
t0 <- qt(1 - alpha/2, n-2)
ogbeta0 <- b0 - t0*s_b0
bgbeta0 <- b0 + t0*s_b0
ogbeta1 <- b1 - t0*s_b1
bgbeta1 <- b1 + t0*s_b1
@
\begin{equation}
  \begin{split}
  \Sexpr{round(ogbeta0,3)} &< \beta_{0} < \Sexpr{round(bgbeta0,3)} \\
  \Sexpr{round(ogbeta1,3)} &< \beta_{1} < \Sexpr{round(bgbeta1,3)}
  \end{split}
\end{equation}

We kunnen ook de nulhypothese $H_{0}: \beta_{1}=0$ testen. De alternatieve hypothese is $H_{a}: \beta_{1} \neq 0$. Het is een tweezijdige toets. 

De toetsveranderlijke is $\phi^{*}=\frac{(b_{1}-\beta_{1})}{s_{b_{1}}}$

Deze is $T_{(n-2)}$-verdeeld. De p-waarde is dan gelijk aan $p=2*(1-F_{T}(|\phi^{*}|,(n-2)))=$\Sexpr{round(2*(1-pt(abs(b1/s_b1),n-2)),3)}.

\subsection{Kwaliteit van het model}
\newthought{De determinatieco\"effici\"ent}.\\

We splitsen $SSTO$ op in een som van $SSR$ en $SSE$. $SSR$ is de variantie die verklaard wordt door het model, $SSE$ het onverklaarde deel. De determinatieco\"effici\"ent is de verhouding van $SSR$ t.o.v. $SSTO$ en ligt tussen 0 (geen verband) en 1 (perfect lineair verband). De formules:
\begin{equation}
  \begin{split}
  SSTO &= \sum\limits_{i=1}^{i=n}(y_{i}-\bar{y})^{2} \\
  SSR &= \sum\limits_{i=1}^{i=n}(\hat{y}_{i}-\bar{y})^{2} \\
  SSE &= \sum\limits_{i=1}^{i=n}(\hat{y}_{i}-y_{i})^{2} \\
  SSTO &= SSR + SSE \\
  R^{2} &= \frac{SSR}{SSTO}=1-\frac{SSE}{SSTO} \\
  R^{2} &= b_{1}^{2}\frac{SS_{xx}}{SS_{yy}}
  \end{split}
\end{equation}

<<echo=FALSE>>=
R_sq <- (b1^2)*ss_xx/ss_yy
@

In dit voorbeeld is $R^{2}=$\Sexpr{round(R_sq,4)}.

Al het rekenwerk en de tussenresultaten zijn beschikbaar in een lijst die geproduceerd wordt wanneer we R een lineair model laten opmaken:

<<>>=
lineair_model <- lm(y ~ x, data=geg)
summary(lineair_model)
names(lineair_model)
@

\medskip
Het is bekend dat we de determinatieco\"effici\"ent $R^{2}$ artificieel kunnen verhogen door bijkomende onafhankelijke variabelen in te voeren. Bij drie waarnemingen kunnen we komen tot een perfect functioneel verband door naast $x$ ook $x^{2}$ als variabele op te nemen. In het algemeen zal een model \emph{gesatureerd} zijn als het aantal variabelen gelijk is aan $(n-1)$.

Daarom gaat met de determinatieco\"effici\"ent aanpassen om rekening te houden met het aantal variabelen:
\begin{equation}
R_{adj}^2=1-(1-R^{2})\frac{n-1}{n-p-1}
\end{equation}
met $p$ gelijk aan het aantal modelparameters (de constante term niet inbegrepen), of $(m-1)$. In dit voorbeeld is $m=2$ en dus $p=1$ zodat
\begin{equation}
R_{adj}^2=1-(1-R^{2})\frac{n-1}{n-2}=1-(1-R^{2})\frac{4}{3}
\end{equation}

In dit voorbeeld is $R_{adj}^{2}=$\Sexpr{round((1-(1-R_sq)*(n-1)/(n-2)),4)}.

\newthought{Je kan de significantie} van het lineair model toetsen door een Analysis Of Variance (anova) te doen van het lineair model. Daarin krijg je een overzicht van de vrijheidsgraden, de sums of squares ($SSR$ en $SSE$), de mean sums of squares ($MSR$ en $MSE$), de daaruit afgeleide F-waarde en de p-waarde.

<<>>=
anova(lineair_model)
@

\subsection{Voorspellingen gebruik makend van het lineair model}
\newthought{Voorspellen van $E(Y_{h})$}

We nemen een waarde van de onafhankelijk variabele $x=x_{h}$ en berekenen hiermee de waarde $\hat{y}_{h}=b_{0}+b_{1}x_{h}$. Het model doet een voorspelling voor $E(Y_{h})$, niet voor $y_{h}$ (dat bekijken we in volgende paragraaf). Omdat $B_{0}$ en $B_{1}$ kansvariabelen zijn (normaal verdeeld) zal $Y_{h}$ ook normaal verdeeld zijn met:\sidenote{Goos, pp. 28}
\begin{equation}
  \begin{split}
  E(Y_{h})&=E(B_{0}+B_{1}x_{h})=\beta_{0}+\beta_{1}x_{h} \\
  var(Y_{h})&= \sigma^{2}\left( \frac{1}{n} + \frac{(x_{h}-\bar{x})^{2}}{SS_{xx}} \right)
  \end{split}
\end{equation}

Dat betekent dat de variabele $\frac{(Y_{h}-E(Y_{h}))}{\sqrt{\sigma^{2}\left( \frac{1}{n} + \frac{(x_{h}-\bar{x})^{2}}{SS_{xx}}\right)}}$ T(n-2) verdeeld is, waarmee we een betrouwbaarheidsinterval kunnen berekenen voor $E(Y_{h})$:

\begin{equation}
  \begin{split}
  t_{0}&=F_{T}^{-1}(1-\frac{\alpha}{2}, n-2) \\
  \hat{Y}_{h}-t_{0}\sqrt{s^{2}\left( \frac{1}{n} + \frac{(x_{h}-\bar{x})^{2}}{SS_{xx}}\right)} &< E(Y_{h}) < \hat{Y}_{h}+t_{0}\sqrt{s^{2}\left( \frac{1}{n} + \frac{(x_{h}-\bar{x})^{2}}{SS_{xx}}\right)}
  \end{split}
\end{equation}

<<echo=FALSE>>=
x_h <- c(1:n)
betrouwbaarheid <- 0.95
alpha <- 1 - betrouwbaarheid
t0 <- qt(1 - alpha/2, n-2)
s_Eyh <- s*sqrt((1/n + ((x_h - gem_x)^2)/ss_xx))
Eyh <- b0 + b1*x_h
ogEyh <- Eyh - t0*s_Eyh
bgEyh <- Eyh + t0*s_Eyh
grenzen_Eyh <- data.frame(xh=x_h, fit= Eyh, ondergrens=ogEyh, bovengrens=bgEyh)
@

<<>>=
grenzen_Eyh
@
\medskip
In R gebruiken we de \emph{predict} functie: je start met de $x_{h}$-waarden waarvoor je een voorspelling van $E(y_{h})$ wil. Die moeten in een data-frame geplaatst met als kolomnaam dezelfde naam als diegene die gebruikt werd voor de onafhankelijke veranderlijke bij de opstelling van het lineair model (in dit gevel $x$). Je kan dan de voorspelde waarde(n) en de $(1-\alpha)$ onder- en bovengrens berekenen. Bijkomende info is mogelijk bv. de standard error.

<<echo=FALSE>>=
pred_xh <- data.frame(x=x_h)
pred_yh <- as.data.frame(predict(lineair_model, pred_xh, interval= "confidence", level=0.95))
pred_yh$xh <- pred_xh$x
pred_yh <- pred_yh[, c(4,1,2,3)]
@
<<>>=
pred_yh
@
\newthought{Wanneer je de waarde van een nieuw experiment} $y_{h}$ wil voorspellen dan volgt uit:
\begin{equation}
Y_{h}=E(Y_{h}) + U_{h} = \hat{Y}_{h} + U_{h}
\end{equation}

dat $Y_{h}$ ook normaal zal verdeeld zijn omdat $U \sim N(0, \sigma)$ verdeeld is. De gemiddelde waarde van $Y_{h}$ zal dus ook gelijk zijn aan $\hat{Y}_{h}$ en de variantie van $Y_{h}$ zal gelijk zijn aan de variantie van $E(Y_{h})$ plus de variantie van $U_{h}=\sigma^{2}$. Het betrouwbaarheidsinterval voor $Y_{h}$ wordt dus wat groter:
\begin{equation}
  \begin{split}
  t_{0}&=F_{T}^{-1}(1-\frac{\alpha}{2}, n-2) \\
  \hat{Y}_{h}-t_{0}\sqrt{s^{2}\left(1 + \frac{1}{n} + \frac{(x_{h}-\bar{x})^{2}}{SS_{xx}}\right)} &< Y_{h} < \hat{Y}_{h}+t_{0}\sqrt{s^{2}\left(1 + \frac{1}{n} + \frac{(x_{h}-\bar{x})^{2}}{SS_{xx}}\right)}
  \end{split}
\end{equation}

<<echo=FALSE>>=
s_yh <- s*sqrt((1 + 1/n + ((x_h - gem_x)^2)/ss_xx))
ogyh <- Eyh - t0*s_yh
bgyh <- Eyh + t0*s_yh
grenzen_yh <- data.frame(xh=x_h, fit=Eyh, ondergrens=ogyh, bovengrens=bgyh)
@

<<>>=
grenzen_yh
@
\medskip
In R gebruiken we opnieuw de \emph{predict} functie met als keuze "predict" voor de parameter \emph{interval}:

<<echo=FALSE>>=
pred_y <- as.data.frame(predict(lineair_model, pred_xh, interval= "predict", level=0.95))
pred_y$xh <- pred_xh$x
pred_y <- pred_y[, c(4,1,2,3)]
@

<<>>=
pred_y
@

\subsection{Gebruik van package ''broom"}
Dit package omvat 3 functies: ''tidy", ''augment" en ''glance". Hierdoor wordt de informatie die in het model object zit (meestal een lijst) omgezet naar een data.frame zodat je daar veel gemakkelijker onderdelen uit kan oproepen.

<<>>=
broom::tidy(lineair_model)
broom::augment(lineair_model)
linmod <- augment(lineair_model) #make model results available in data.frame
broom::glance(lineair_model)
@
\newpage
\section{Controle van de veronderstellingen}

\subsection{Veronderstelling 1: $E(U_{i})=0$}
We hebben geen directe toegang tot de kansvariabele $U_{i}$. De residus $r_{i}$ zijn beschikbaar, maar door de aard van de gebruikte methode (kleinste kwadraten) zal de gemiddelde waarde $E(r_{i})$ altijd gelijk aan nul zijn, zodat we hieruit geen bijkomende info kunnen halen. Wanneer de fouten ontstaan door verschillende factoren waarvan er niet \'e\'en overheersend is, dan kunnen we de Centrale Limietstelling inroepen. Dit betekent echter niet automatische dat $E(U_{i})=0$.

\subsection{Veronderstelling 2: $E(Y)=\beta_{0}+\beta_{1}x$}
Deze veronderstelling kan getoests worden door een scatterplot te maken van de residus $r_{i}$ i.f.v. de waarden van de onafhankelijke variabele $x_{i}$ of i.f.v. de berekende waarden $\hat{y_{i}}$. Indien de veronderstelling van lineariteit correct is dan moeten, als $E(U_{i})=0$ en $var(U_{i})=constant$, de residus $r_{i}$ willekeurig verdeeld zijn rondom de nullijn. Er mag geen patroon zichtbaar zijn. Dezelfde grafiek wordt ook gebruikt om na te gaan of voldaan is aan veronderstelling 3. 
@

\subsection{Veronderstelling 3: $var(U_{i})=\sigma^{2}=constant$}
\label{subsec:Veronderstelling3}
De waarden voor $u_{i}$ zijn onbekend omdat $u_{i}=y_{i}-\beta_{0}-\beta_{1}x_{i}$, en $\beta_{0}$ en $\beta_{1}$ niet gekend zijn. We kennen wel de waarden van de residus $r_{i}$, maar meestal slechts \'e\'en waarde $r_{i}$ voor elke waarde van $x_{i}$. We kunnen een grafiek maken van $r_{i}$ i.f.v. $x_{i}$ (Figuur~\ref{fig:controle_veronderstelling3}-Links). Daarop kan je zien of er een patroon zichtbaar is dat afwijkt van ''chaos rondom de nullijn(=het gemiddelde van residus en fouten)". Dat zou een indicator kunnen zijn dat $\sigma$ niet constant is. Je kan als x-as i.p.v. de onafhankelijke variabele $x$ ook de berekende waarde $\hat{y}$ nemen: dat geeft hetzelfde beeld als er een lineair verband is tussen beide (Figuur~\ref{fig:controle_veronderstelling3}-Midden). Tenslotte kan men ook i.p.v. met de residus $r_{i}$ te werken, gestandaardiseerde residus $\frac{r_{i}}{s}$ gebruiken. Die zijn dimensieloos en onafhankelijk van de aard en grootte van de responsvariabele $y$. De interpretatie van de grafiek is dan onafhankelijk van de concrete situatie: we gaan na of er residus zijn die b.v. meer dan 2 standaardafwijkingen groot zijn (Figuur~\ref{fig:controle_veronderstelling3}-Rechts).

Package \textbf{car} bevat een \emph{goodness of fit}-test voor ''non-constant error variance":
<<>>=
ncvTest(lineair_model)
@

<<label=controle_veronderstelling3,fig=TRUE,include=FALSE, echo=FALSE>>=
p1 <- ggplot(data=geg) +
  geom_point(aes(x = x, y = r)) +
  geom_hline(yintercept=0, col="blue", linetype="dashed") +
  xlim(0, 6) +
  ylim(-2, 2) +
  labs(title = "Veronderstelling 3", 
       subtitle = "Constante variantie",
       x = "onafhankelijke variabele x", 
       y = "residu r") +
  JT.theme
p2 <- ggplot(data=geg) +
  geom_point(aes(x = yhat, y = r)) +
  geom_hline(yintercept=0, col="blue", linetype="dashed") +
  xlim(0, 5) +
  ylim(-2, 2) +
  labs(title = "Veronderstelling 3", 
       subtitle = "Constante variantie",
       x = "berekende waarde van y", 
       y = "residu r") +
  JT.theme
geg$r_stand <- geg$r/s
p3 <- ggplot(data=geg) +
  geom_point(aes(x = yhat, y = r_stand)) +
  geom_hline(yintercept=0, col="blue", linetype="dashed") +
  geom_hline(yintercept=-2, col="red") +
  geom_hline(yintercept=2, col="red") +
  xlim(0, 5) +
  ylim(-3, 3) +
  labs(title = "Veronderstelling 3", 
       subtitle = "Constante variantie",
       x = "berekende waarde van y", 
       y = "gestandaardiseerde residu r") +
  JT.theme
grid.arrange(p1, p2, p3, ncol=3)
@

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{LinReg-controle_veronderstelling3}
\caption{ }
\label{fig:controle_veronderstelling3}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{figure}

\newthought{Maar hier zit een adder onder het gras} nl. de intu\"itieve veronderstelling dat de variantie van de residus $r_{i}$ constant is en gelijk aan $\sigma^{2}$. Dat dit niet zo is blijkt uit volgende simulatie waarbij we N (1000) steekproeven doen voor n (5) instellingen van de onafhankelijke variabele $x_{i}$. Voor elke steekproef worden de kleinste kwadraten-rechten berekenen en de residus. Voor elke waarde van $x_{i}$ kan je dan de variantie van de residus bepalen.

<<echo=FALSE>>=
Nsim <- 100
nsim <- 5
b0sim <- -1
b1sim <- 7
sigma_U <- 2
set.seed(2018)
simul1 <- data.frame(expnr = rep((1:Nsim), each=nsim), x = rep(seq(1,nsim),Nsim), y = 0)
simul1$y <- b0sim + b1sim*simul1$x + rnorm(nsim*Nsim, 0, sigma_U)
simul1_mod <- data.frame(expnr=seq(1:Nsim), b0=0, b1=0)
for (i in 1:Nsim) {
  simul1 %>% filter(expnr==i) %>% lm(y~x,.) %>% tidy() -> tidytabel
  simul1_mod[i,2] <- tidytabel$estimate[1]
  simul1_mod[i,3] <- tidytabel$estimate[2]
}
simul1$yhat <- simul1_mod[simul1$expnr,2] + simul1_mod[simul1$expnr,3]*simul1$x 
simul1$resid <- simul1$y - simul1$yhat 
@

<<>>=
simul1 %>% group_by(x) %>% summarise(variantie=var(resid))
@

Wat opvalt is dat de variantie van de residus \emph{kleiner} is aan de uiteinden dan in het midden! Dit komt omdat de meetpunten aan de uiteinden een grotere invloed (\emph{leverage}) hebben bij het bepalen van $b_{0}$ en $b_{1}$. Hierdoor ligt de rechte $b_{0}+b_{1}x$ dichter bij de meetwaarden $y_{i}$ aan de uiteinden, waardoor ook het residu daar kleiner wordt. De variantie van de residus $r_{i}$ is dus niet enkel afhankelijk van $\sigma$ maar ook van de invloed (\emph{leverage}) dat punt ($x_{i}, y_{i}$) heeft op de berekening van de parameters van de rechte. De meer extreme punten zijn diegene waarvoor $(x_{i}-\bar{x})$ groot is. De \emph{leverage} kunnen we halen uit de \emph{hat-matrix H}.

\newthought{De hat-matrix H} komt naar voor wanneer we een matrixnotatie gebruiken voor de residus. Dat maakt uitbreiding naar lineaire mo-dellen met meer dan \'e\'en onafhankelijke variabele zeer gemakkelijk. In formulevorm zijn de residus gegeven door:
\begin{equation}
r_{i}=y_{i}-\hat{y}_{i}=y_{i}-b_{0}-b_{1}x_{i} \quad i=1 \ldots n
\end{equation}

Je kan dat in matrixvorm schrijven als:
\begin{equation}
\boldsymbol{r}=\boldsymbol{Y}-\boldsymbol{\hat{Y}}=\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{b}
\end{equation}

met
\begin{equation}
\boldsymbol{r}=
\begin{bmatrix} 
r_{1} \\
r_{2} \\
r_{3} \\
r_{4} \\
r_{5} 
\end{bmatrix}
\quad
\boldsymbol{Y}=
\begin{bmatrix} 
y_{1} \\
y_{2} \\
y_{3} \\
y_{4} \\
y_{5} 
\end{bmatrix}
\quad
\boldsymbol{\hat{Y}}=
\begin{bmatrix} 
\hat{y}_{1} \\
\hat{y}_{2} \\
\hat{y}_{3} \\
\hat{y}_{4} \\
\hat{y}_{5} 
\end{bmatrix}
\quad
\boldsymbol{X}=
\begin{bmatrix} 
1 & x_{1} \\
1 & x_{2} \\
1 & x_{3} \\
1 & x_{4} \\
1 & x_{5} 
\end{bmatrix}
\quad
\boldsymbol{b}=
\begin{bmatrix} 
b_{0} \\
b_{1}
\end{bmatrix}
\end{equation}

Je kan bewijzen\sidenote{Goos, pp. 34} dat als $\boldsymbol{X'}\boldsymbol{X}$ inverteerbaar is, dat
\begin{equation}
  \boldsymbol{b}=\left( \boldsymbol{X'}\boldsymbol{X} \right)^{-1}\boldsymbol{X'}\boldsymbol{Y}
\end{equation}

zodat
\begin{equation}
\boldsymbol{r}=\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{b}=\boldsymbol{Y}-\boldsymbol{X}\left( \boldsymbol{X'}\boldsymbol{X} \right)^{-1}\boldsymbol{X'}\boldsymbol{Y}=\left[ \boldsymbol{I} - \boldsymbol{X}\left( \boldsymbol{X'}\boldsymbol{X} \right)^{-1}\boldsymbol{X'} \right]\boldsymbol{Y}
\end{equation}
We stellen
\begin{equation}
\boldsymbol{H}=\boldsymbol{X}\left( \boldsymbol{X'}\boldsymbol{X} \right)^{-1}\boldsymbol{X'}
\end{equation}

zodat
\begin{equation}
  \begin{split}
  \boldsymbol{r}&=\left[ \boldsymbol{I} - \boldsymbol{H} \right]\boldsymbol{Y} \\
  en \\
  \boldsymbol{\hat{Y}}&=\boldsymbol{H}\boldsymbol{Y}
  \end{split}
\end{equation}

Hieruit volgt dat, bij voorbeeld, de tweede berekende waarde $\hat{y}_{2}$ gelijk zal zijn aan
\begin{equation}
\hat{y}_{2}=h_{2,1}y_{1}+h_{2,2}y_{2}+h_{2,3}y_{3}+h_{2,4}y_{4}+h_{2,5}y_{5}
\end{equation}

$h_{2,2}$ zal dus bepalend zijn voor de mate waarin het meetpunt $y_{2}$ de berekende waarde $\hat{y}_{2}$ zal be\"invloeden. Dit noemen we de \emph{leverage} of de \emph{hefboomwerking}. De som van alle hefboomwaarden is altijd gelijk aan \emph{het aantal modelparameters m}. Bij een enkelvoudige regressie zijn dat er twee: $b_{0}$ en $b_{1}$.
<<echo=FALSE>>=
X <- matrix(c(rep(1,n),geg$x), ncol=2)
H <- X%*%solve(t(X)%*%X)%*%t(X)
@

<<>>=
sum(diag(H))
@

Indien elke waarneming hetzelfde belang zou hebben dan zouden de diagonaalwaarden overal gelijk moeten zijn aan het aantal mo-delparameters gedeeld door $n$. In dit geval is dat \Sexpr{round(sum(diag(H))/n,3)}. Wanneer we de matrix $\boldsymbol{H}$ bekijken dat is dat niet het geval: \emph{de eindpunten} hebben een hoger dan gemiddelde hefboom, \emph{de middelpunten} hebben een lager dan gemiddelde hefboom.

<<>>=
H
@
\medskip
De variantie van de residus wordt dus ook bepaald door de hefboomwaarden:
\begin{equation}
var(r_{i})=\sigma^{2}\left( 1-h_{i,i} \right)
\end{equation}
en we zien dus dat de variantie aan de uiteinden wat lager is. Om residus op een correcte manier met elkaar te vergelijken gaan we ze standaardiseren door het gemiddelde (0) af te trekken, en te delen door de juiste standaardafwijking. Dat noemen we de \emph{studentized} residus, omdat zij kansvariabelen zijn die T(n-1-m) verdeeld zijn.
\begin{equation}
stdres_{i}=\frac{r_{i}}{\sqrt{\sigma^{2}\left(1-h_{i,i} \right)}}
\end{equation}

$\sigma^{2}$ is niet bekend maar wordt geschat door $MSE=\frac{SSE}{(n-2)}$ zodat
\begin{equation}
stdres_{i}=\frac{r_{i}}{\sqrt{MSE\left(1-h_{i,i} \right)}}
\end{equation}

In dit voorbeeld zijn de studentized residus dus:
<<echo=FALSE>>=
geg$hdiag <- diag(H)
geg$student_res <- geg$r/sqrt((sse/(n-2))*(1-geg$hdiag))
@
<<>>=
geg$student_res
@

Deze waarden zijn direct beschikbaar in het lineaire model. Met het ''broom"-package maken we die beschikbaar in een data.frame:
<<>>=
linmod$.std.resid
@

De relevante grafiek wordt dus het verband tussen de studentized residus i.f.v. de onafhankelijke variabele $x_{i}$ of de berekende waarde $\hat{y}_{i}$ (Figuur~\ref{fig:studentized_resid}). 

<<label=studentized_resid,fig=TRUE,include=FALSE, echo=FALSE>>=
xmin <- min(linmod$.fitted) - 0.2*(abs(min(linmod$.fitted)))
xmax <- max(linmod$.fitted) + 0.2*(abs(max(linmod$.fitted)))
ymin <- min(c(-3,min(linmod$.std.resid) - 0.2*(abs(min(linmod$.std.resid)))))
ymax <- max(c(3,max(linmod$.std.resid) + 0.2*(abs(max(linmod$.std.resid)))))
p1 <- ggplot(data=linmod, aes(x = .fitted, y = .std.resid)) +
  geom_point() +
  geom_text(label=rownames(linmod), nudge_x = 0.2*(abs(min(linmod$.fitted))), nudge_y = 0, size=2.5) +
  geom_hline(yintercept=0, col="blue") +
  geom_hline(yintercept=-2, col="red", linetype="dashed") +
  geom_hline(yintercept=2, col="red", linetype="dashed") +
  xlim(xmin, xmax) +
  ylim(ymin, ymax) +
  labs(title = "Constante variantie", 
       x = "Berekende waarde van y", 
       y = "Studentized Residu r") +
  JT.theme
p1
@

\begin{marginfigure}[-2cm]
\centering
\includegraphics[width=1\textwidth]{LinReg-studentized_resid}
\caption{ }
\label{fig:studentized_resid}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

Het nadeel van Figuur~\ref{fig:studentized_resid} is dat je zowel oog moet hebben voor positieve als negatieve residus. Je kan je een trechtergrafiek voorstellen waarbij zowel de positieve als de negatieve residus stijgen i.f.v. $\hat{y}_{i}$ zodat de variantie stijgt, terwijl de gemiddelde waarde toch nul blijft (de blauwe lijn). Daarom gebruikt men soms het kwadraat van de (studentized) residus, of de absolute waarde. In een ''Scale-Location''-plot gebruikt men de wortel van de absolute waarde van de residus $|r_{i}|$ die uitgezet wordt in functie van de berekende waarde $y_{i}$ (Figuur~\ref{fig:scale_location})\sidenote{Men neemt de vierkantswortel van de absolute waarden van de residus omdat de scheefheid daarvan kleiner is dan de scheefheid van de absolute waarde op zich}. Die waarden zouden willekeurig rondom een horizontale lijn moeten liggen (niet de nullijn). Indien dat niet het geval is dan zien we in de grafiek op welke \emph{plaats} (location) de residus, en daarom wellicht ook de variantie, groter is (scale).

<<label=scale_location,fig=TRUE,include=FALSE, echo=FALSE>>=
#  lm_abs_stud_res <- lm(abs(student_res) ~ yhat, data=geg)
ymin <- -0.2*max(sqrt(abs(linmod$.std.resid))) 
ymax <- 1.2*max(sqrt(abs(linmod$.std.resid))) 
p3 <- ggplot(data=linmod, aes(x = .fitted, y = sqrt(abs(.std.resid)))) +
  geom_point() +
  geom_text(label=rownames(linmod), nudge_x = 0, nudge_y = 0.1*max(sqrt(abs(linmod$.std.resid))), size=2.5) +
  xlim(xmin, xmax) +
  ylim(ymin, ymax) +
  geom_hline(yintercept=0, col="blue") +
  labs(title = "Scale-Location", 
       x = "Berekende waarde van y", 
       y = expression(sqrt(abs("Studentized Residu r")))) +
  JT.theme
p3
@

\begin{marginfigure}[0cm]
\centering
\includegraphics[width=1\textwidth]{LinReg-scale_location}
\caption{ }
\label{fig:scale_location}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

\subsection{Veronderstelling 4: $cov(U_{i},U_{j})=0$ voor $i \neq j$}
Tricky! Als er onderlinge afhankelijkheid is, dan is het meestal via een derde element, bij voorbeeld \'e\'en van de onafhankelijke variabelen. In dit geval is er maar \'e\'en onafhankelijke variabele en vallen we terug op Figuur~\ref{fig:controle_veronderstelling3}. Daar kunnen we eventueel zien of er een verband is tussen $u_{i}$ en $x_{i}$. Een anders situatie is autocorrelatie waarbij de huidige term afhankelijk is van de vorige. Dat komt veel voor bij tijdsreeksen. Daar is het dus het type van de data (tijdsreeks) dat doet twijfelen aan Veronderstelling 4. De Durbin-Watson test (in het ''car"-package) gaat na of er een lag1-autocorrelatie is en, voor die situatie, kunnen we dit nagaan voor de fouttermen:

<<>>=
durbinWatsonTest(lineair_model)
@

In dit voorbeeld kunnen we autocorrelatie lag1 uitsluiten.

\subsection{Veronderstelling 5: $U \sim N(0,\sigma)$}
Dit kunnen we eenvoudig zien met ''qqnorm" en ''qqline" in R. Opgeschoond met ggplot geeft dit Figuur~\ref{fig:controle_veronderstelling5}. We zien dat de gemiddelde waarde van de residus (Veronderstelling 1) inderdaad nul is. De normaliteit wordt bevestigd door de ligging t.o.v. de ideale lijn.

<<label=controle_veronderstelling5,fig=TRUE,include=FALSE, echo=FALSE>>=
kwantiel <- data.frame(q1=qqnorm(linmod$.std.resid, plot.it = FALSE)[[1]], 
                       q2=qqnorm(linmod$.std.resid, plot.it = FALSE)[[2]])
xmin <- min(kwantiel$q1) - 0.2*(abs(min(kwantiel$q1)))
xmax <- max(kwantiel$q1) + 0.1*(abs(max(kwantiel$q1)))
ymin <- min(kwantiel$q2) - 0.2*(abs(min(kwantiel$q2)))
ymax <- max(kwantiel$q2) + 0.1*(abs(max(kwantiel$q2)))
y1 <- quantile(linmod$.std.resid, 0.25)
y2 <- quantile(linmod$.std.resid, 0.75)
x1 <- qnorm(0.25)
x2 <- qnorm(0.75)
slope <- (y2-y1)/(x2-x1)
interc <- y1 - x1*slope
p2 <- ggplot(kwantiel, aes(q1, q2)) +
  geom_point(na.rm = TRUE, size=1) +
  geom_text(label=rownames(kwantiel), nudge_x = (xmax-xmin)/40, nudge_y = 0, size=2.5) +
  geom_abline(slope=slope, intercept=interc, color="blue") +
  xlim(min(xmin,ymin), max(xmax,ymax)) +
  ylim(min(xmin,ymin), max(xmax,ymax)) +
  xlab("Theor. Studentized Quantiles") +
  ylab("Exp. Studentized Quantiles") +
  ggtitle("Normplot studentized residus") +
  JT.theme
p2
@

\begin{marginfigure}[2cm]
\includegraphics[width=1\textwidth]{LinReg-controle_veronderstelling5}
\caption{ }
\label{fig:controle_veronderstelling5}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

De grafische methode heeft het probleem dat de beslissing over al dan niet normaliteit met het oog wordt genomen. Wat is ''dicht bij de ideale lijn"? Er zijn ook verschillende normaliteitstesten. Ik gebruik het R-package \textbf{nortest}.

<<>>=
shapiro.test(linmod$.std.resid)
ks.test(linmod$.std.resid, pnorm)
# if (n>6) {ad.test(linmod$.std.resid)}
@

\subsection{Invloedrijke meetwaarden}
\newthought{De invloed} van meetwaarden die ver gelegen zijn t.o.v. het gemiddelde $\bar{x}$ is groter dan de invloed van punten die naar het midden toe gelegen zijn. Dat hebben we gezien als \emph{leverage}, en een maat hiervoor zijn de waarden op de diagonaal van de ''hat-matrix" $H$. We spreken van een grote leverage wanneer de h-waarde meer dan twee keer het gemiddelde $\bar{h}=\frac{m}{n}$ waarbij $m$ het aantal modelparameters is (hier 2) en $n$ het aantal meetpunten (hier 5). Een voorwaarde is wel dat het aantal meetpunten $n$ voldoende groot moet zijn, wat in dit voorbeeld niet het geval is.

Wanneer we vaststellen dat \'e\'en of meerdere meetpunten een leverage hebben die ''groot" is, dan moeten we nog nagaan of dat hefboomeffect een invloed heeft op het model. Wanneer een meetpunt ver van het gemiddelde gelegen is, maar vrijwel op de regressielijn die bepaald wordt door de andere punten, dan is de invloed van dat meetpunt gering en hoeven we ons geen zorgen te maken dat dit meetpunt het model sterk be\"invloedt. Een maat hiervoor is \emph{Cook's Distance}. De Cook's Distance van meetpunt i wordt gegeven door:
\begin{equation}
D_{i}=\frac{\sum\limits_{j=1}^{j=n}\left( \hat{y}_{j} - \hat{y}_{j(i)} \right)}{mMSE}
\end{equation}

Hierin is $\hat{y}_{j}$ de berekende waarde bij $x_{j}$ als alle $n$ meetpunten in rekening worden gebracht, terwijl $\hat{y}_{j(i)}$ de berekende waarde is bij $x_{j}$ als we meetpunt $i$ \emph{niet} in de berekening opnemen. Dat doen we voor alle meetpunten en we nemen de som. Om dit te standaardiseren delen we door het aantal modelparameters $m$ vermenigvuldigd met MSE. Deze statistiek is F(n,n-m) verdeeld. Als de kans dat we deze waarde of hoger vinden groter is dan 50\% dan heeft de betrokken waarneming een grote invloed op het model.

De berekening van Cook's Distance lijkt ingewikkeld omdat men telkens een bijkomend lineair model moet opstellen waarbij de i-de waarneming wordt uitgesloten. Gelukkig kan men dit herleiden tot een eenvoudigere berekening waarbij we enkel de residus $r_{i}$ en de diagonaalelementen $h_{ii}$ van de hat-matrix $H$ nodig hebben:
\begin{equation}
D_{i}=\frac{r_{i}^{2}}{mMSE}\left[ \frac{h_{ii}}{\left( 1 - h_{ii} \right)^{2}} \right]
\end{equation}

<<>>=
m <- 2
MSE=sse/(n-2)
geg$Di <- ((geg$r^2)/(m*MSE))*(geg$hdiag/(1-geg$hdiag)^{2})
geg$Di
@

We vinden die ook terug in de resultaten van het lineair model (Figuur~\ref{fig:Cooks_Distance}).
<<>>=
linmod$.cooksd
@

<<label=Cooks_Distance,fig=TRUE,include=FALSE, echo=FALSE>>=
p4 <- ggplot(linmod, aes(seq_along(.cooksd))) +
  geom_linerange(aes(ymin = rep(0,n), ymax = .cooksd), size=0.5) +
  geom_hline(yintercept=(4/(n-m)), col="red") +
  xlab("Meetpunt") +
  ylab("Cook's distance") +
  ggtitle("Invloedrijke meetpunten", subtitle="Cook's distance") +
  JT.theme
p4
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{LinReg-Cooks_Distance}
\caption{ }
\label{fig:Cooks_Distance}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}
\medskip
De kans dat we deze Cook's Distances (of groter) vinden wordt berekend met de $F_{m,(n-m)}$-kansverdeling:
\begin{equation}
P[D_{i} of groter]=1-F_{F}(D_{i})=1-pf(D_{i},m,n-m)
\end{equation}

<<>>=
geg$kansDi <- 1-pf(geg$Di,m,n-m)
geg$kansDi
@

Andere criteria voor ''invloedrijkheid" zijn:
\begin{itemize}
  \item $D_{i} > 1$
  \item $D_{i} > \frac{4}{(n-m)}$
\end{itemize}

In Figuur~\ref{fig:Cooks_Distance} is het laatste criterium opgenomen als rode grenslijn.
\medskip
De formule voor Cook's Distance
\begin{equation}
D_{i}=\frac{r_{i}^{2}}{mMSE}\left[ \frac{h_{ii}}{\left( 1 - h_{ii} \right)^{2}} \right]
\end{equation}

geeft duidelijk aan wanneer een meetpunt invloedrijk is, en dus de moeite om grondig te bekijken (niet om het te verwijderen!). De invloed van meetpunt $i$ is groot wanneer het residu $r_{i}$ groot is en/of de leverage $h_{ii}$ groot is. Een punt dat ver gelegen is van het zwaartepunt $(\bar{x},\bar{y})$ heeft een grote hefboom t.o.v. het draaipunt $(\bar{x},\bar{y})$ en kan daardoor de regressierechte meer verdraaien door haar invloed op $b_{0}$ en $b_{1}$. Maar indien dat meetpunt gelegen is op of in de buurt van de regressielijn (klein residu $r_{i}$) dan zal de invloed (Cook's Distance) toch klein zijn. Analoog zal een meetpunt dat dicht bij het zwaartepunt ligt toch een grote invloed kunnen hebben op de ligging van de regressielijn wanneer het ver van die lijn (die in grote mate door de (n-1) andere meetpunten wordt bepaald) en dus een groot residu $r_{i}$ heeft. Het is dus zinvol om die twee effecten (residu en leverage) uit elkaar te halen. Dat kan door het residu (studentized) uit te zetten i.f.v. de leverage (Figuur~\ref{fig:Cooks_explained} links) of de Cook's Distance i.f.v. de breuk $h_{ii}$ (Figuur~\ref{fig:Cooks_explained} rechts).

<<label=Cooks_explained,fig=TRUE,include=FALSE, echo=FALSE>>=
xmin <- min(linmod$.hat) - 0.2*(abs(min(linmod$.hat)))
xmax <- max(2*m/n,max(linmod$.hat)) + 0.1*(abs(max(linmod$.hat)))
ymin <- min(c(-3,min(linmod$.std.resid) - 0.2*(abs(min(linmod$.std.resid)))))
ymax <- max(c(3,max(linmod$.std.resid) + 0.1*(abs(max(linmod$.std.resid)))))
p5 <- ggplot(linmod, aes(.hat, .std.resid)) +
  geom_point(aes(size=.cooksd), na.rm=TRUE) +
  geom_text(label=rownames(linmod), nudge_x = 0.2*(abs(min(linmod$.hat))), nudge_y = 0, size=2.5) +
  geom_vline(xintercept=2*m/n, col="red") +
  xlim(xmin,xmax) +
  ylim(ymin,ymax) +
  xlab("Leverage hii") +
  ylab("Studentized Residus") +
  ggtitle("Invloedrijke meetpunten", subtitle="Residu ri i.f.v. leverage hii") +
  scale_size_continuous("Cook's Distance", range=c(1,5)) +
  JT.theme +
  theme(legend.position="right", legend.title=element_text(size=7) ,legend.text=element_text(size=5))

ymin <- min(linmod$.cooksd) - 0.2*(abs(min(linmod$.cooksd)))
ymax <- max(linmod$.cooksd) + 0.1*(abs(max(linmod$.cooksd)))   
p6 <- ggplot(linmod, aes(.hat, .cooksd)) +
  geom_point(na.rm=TRUE) +
  geom_vline(xintercept=2*m/n, col="red") +
  geom_text(label=rownames(linmod), nudge_x = 0.2*(abs(min(linmod$.hat))), nudge_y = 0.01, size=2.5) +
  xlim(0,xmax) +
  ylim(ymin,ymax) +
  xlab("Leverage hii") +
  ylab("Cook's Distance") +
  ggtitle("Invloedrijke meetpunten", subtitle="Cook's distance vs hii") +
  geom_abline(slope=seq(0,3,0.5), color="gray", linetype="dashed") +
  JT.theme

grid.arrange(p5, p6,ncol=2)
@

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{LinReg-Cooks_explained}
\caption{ }
\label{fig:Cooks_explained}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{figure}

Aan de linkerkant zien we dat Cook's Distance (oppervlakte van het punt) groter wordt naarmate de leverage $h_{ii}$ en het residu $r_{i}$ groter worden. Aan de rechterkant zien we de lineaire relatie tussen Cook's Distance en $h_{ii}$. De richtingsco\"effici\"ent van de rechte wordt (mede) bepaald door het kwadraat van het residu $r_{i}$. De stippellijnen geven rechten met richtingsco\"effici\"enten gaande van 0 tot 3 in stappen van 0.5. Beide figuren zeggen hetzelfde: meetpunten 1 en 5 (de uitersten) zijn de meest invloedrijke. In beide figuren is de grens $h_{ii}=2\frac{m}{n}$ aangegeven die wijst op grote leverage.


\subsection{Samenvatting van alle diagnostische plots (Figuur~\ref{fig:diagnostic_plots})}

<<label=diagnostic_plots,fig=TRUE,include=FALSE, echo=FALSE>>=
grid.arrange(p1, p2, p3, p4, p5, p6, ncol=2)
@

\begin{figure*}
\centering
\includegraphics[width=1\textwidth]{LinReg-diagnostic_plots}
\caption{Eigen diagnostische ggplots}
\label{fig:diagnostic_plots}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{figure*}

<<label=diagnostic_plotsR,fig=TRUE,include=FALSE, echo=FALSE>>=
par(mfrow = c(3, 2))
plot(lineair_model, which=1, ask=FALSE)
plot(lineair_model, which=2, ask=FALSE)
plot(lineair_model, which=3, ask=FALSE)
plot(lineair_model, which=4, ask=FALSE)
plot(lineair_model, which=5, ask=FALSE)
plot(lineair_model, which=6, ask=FALSE)
@

\begin{figure*}
\centering
\includegraphics[width=1\textwidth]{LinReg-diagnostic_plotsR}
\caption{Standard R-plots of a linear model}
\label{fig:diagnostic_plotsR}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{figure*}

<<>>=
par(mfrow = c(1, 1)) # reset
@

\end{document}