\documentclass{tufte-book}
\usepackage{graphicx}  % werken met figuren
\usepackage{gensymb} % werken met wetenschappelijke eenheden\usepackage{geometry}
\usepackage{changepage} % http://ctan.org/pkg/changepage
\usepackage[dutch,british]{babel} % instelling van de taal (woordsplitsing, spellingscontrole)
\usepackage[parfill]{parskip} % Paragrafen gescheiden door witte lijn en geen inspringing
\usepackage[font=small,skip=3pt]{caption} % Minder ruimte tussen figuur/table en ondertitel. Ondertitel klein
\usepackage{capt-of}
\usepackage{indentfirst}
\setlength{\parindent}{0.7cm}
\usepackage{enumitem} % Laat enumerate werken met letters
\usepackage{url}
\usepackage{lipsum}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
% Prints a trailing space in a smart way.
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{amsmath}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% Alter some LaTeX defaults for better treatment of figures:
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.9}	% max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \renewcommand{\textfraction}{0.1}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.8}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\setcounter{secnumdepth}{3}

\newcommand{\tthdump}[1]{#1}

\newcommand{\openepigraph}[2]{
  \begin{fullwidth}
  \sffamily\large
    \begin{doublespace}
      \noindent\allcaps{#1}\\ % epigraph
      \noindent\allcaps{#2} % author
    \end{doublespace}
  \end{fullwidth}
}


\usepackage{makeidx}
\makeindex

\title{Lineair Regressie}
\author{Jan Trommelmans}

\begin{document}
\SweaveOpts{concordance=TRUE,prefix.string=LinReg}
\setkeys{Gin}{width=1.1\marginparwidth} %% Sweave

<<echo=FALSE>>=
library(tidyverse)
library(broom)
@

% Setting the ggplot theme:
<<echo=FALSE>>=
JT.theme <- theme(panel.border = element_rect(fill = NA, colour = "gray10"),
                  panel.background = element_blank(),
                  panel.grid.major = element_line(colour = "gray85"),
                  panel.grid.minor = element_line(colour = "gray85"),
                  panel.grid.major.x = element_line(colour = "gray85"),
                  axis.text = element_text(size = 8 , face = "bold"),
                  axis.title = element_text(size = 9 , face = "bold"),
                  plot.title = element_text(size = 12 , face = "bold"),
                  strip.text = element_text(size = 8 , face = "bold"),
                  strip.background = element_rect(colour = "black"),
                  legend.text = element_text(size = 8),
                  legend.title = element_text(size = 9 , face = "bold"),
                  legend.background = element_rect(fill = "white"),
                  legend.key = element_rect(fill = "white"))
@

% Functions

\mainmatter

\chapter{Lineaire Regressie}

\section{Inleiding}

We nemen aan dat een responsvariabele $y$ be\"invloed wordt door $n$ onafhankelijke variabelen $x_{i}; i=1 \ldots n$. Maar er zijn ook $k$ onbekende en/of oncontroleerbare storende variabelen $z_{j}; j=1 \ldots k$. We kunnen dit schrijven als:

\begin{equation}
y=f(x_{1}, x_{2}, \ldots x_{n}; z_{1}, z_{2} \ldots z_{k})
\end{equation}

Het gevolg hiervan is dat bij herhaalde experimenten met eenzelfde instelling van de onafhankelijke variabelen $x_{i}$, we andere waarden voor de responsvariabele $y$ zullen krijgen. Figuur~\ref{fig:intro} geeft de situatie waarbij we maar \'e\'en onafhankelijke variabele $x$ hebben die op de waarden 1, 2, 3, 4 en 5 wordt ingesteld bij 10 herhalingsmetingen.
<<label=intro,fig=TRUE,include=FALSE, echo=FALSE>>=
N <- 10
b0 <- -0.1
b1 <- 0.7
sigma_U <- 0.5
set.seed(2018)
geg <- data.frame(expnr = rep((1:10), each=5), x = rep(seq(1,5),N))
geg$y <- b0 + b1*geg$x + rnorm(50, 0, sigma_U)
ggplot(data=geg) +
  geom_point(aes(x = x, y = y, color=expnr)) +
  xlim(0, 6) +
  ylim(0, 6) +
  labs(title = "Startgegevens", 
       subtitle = "10 herhalingsexperimenten",
       x = "onafhankelijke variabele x", 
       y = "waarneming van responsvariabele y") +
  JT.theme
@

\begin{marginfigure}[-6cm]
\includegraphics[width=1\textwidth]{LinReg-intro}
\caption{ }
\label{fig:intro}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

De responsvariabele $y$ is omwille van de storende factoren $z_{j}$ een \emph{kansvariabele} $Y$. We nemen aan dat de gemiddelde waarde van $Y$ beschreven wordt door een functie van de onafhankelijke variabelen $x_{i}$. Dat is een soort Platonisch ideaal dat echter voor ons verborgen is: wij zien enkel de schimmige waarnemingen die verstoord worden door de storende factoren. De verstoring vormt het arbitraire element en is dus het kansgedeelte dat we $U$ noemen en waarvan we aannemen\sidenote{Veronderstelling 1: E(U)=0} dat de gemiddelde waarde $E(U)=0$.
\begin{equation}
  \begin{split}  
  Y=E(Y|x_{1}, x_{2}, \ldots x_{n}) + U&=f(x_{1}, x_{2}, \ldots x_{n}) + U \\
  E(Y|x_{1}, x_{2}, \ldots x_{n})&=f(x_{1}, x_{2}, \ldots x_{n})  + 0
  \end{split}
\end{equation}

\section{Enkelvoudige lineair regressie}

\newthought{Bij een enkelvoudige regressie} is er slechts \'e\'en onafhankelijke variabele x.

\newthought{Het (eenmalige) experiment} geeft volgende resultaten (Figuur~\ref{fig:geg}):
\subsection{Data}
<<label=geg,fig=TRUE,include=FALSE, echo=FALSE>>=
geg <- data.frame(x = seq(1,5), y = c(1, 1, 2, 2, 4))
n <- nrow(geg)
ggplot(data=geg) +
  geom_point(aes(x = x, y = y)) +
  xlim(0, 6) +
  ylim(0, 6) +
  labs(title = "Startgegevens", x = "onafhankelijke variabele x", y = "responsvariabele y") +
  JT.theme
@

\begin{marginfigure}[3cm]
\includegraphics[width=1\textwidth]{LinReg-geg}
\caption{ }
\label{fig:geg}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

<<>>=
geg
@

We veronderstellen\sidenote{Veronderstelling 2: de vorm van de functie f(x) is $E(Y)=\beta_{0} + \beta_{1}x$} dat de achterliggende Platonische waarheid bestaat uit een lineair verband tussen $E(Y)$ end de onafhankelijke variabele $x$:
\begin{equation}
E(Y)=\beta_{0} + \beta_{1}x
\end{equation}

De waarneming $Y$ bij elk experiment is een kansveranderlijke veroorzaakt door de storende factoren en gegroepeerd in de kansveranderlijke U:
\begin{equation}
Y=E(Y)+U=\beta_{0} + \beta_{1}x + U
\end{equation}

De methode van de kleinste kwadraten wordt gebruikt om uit \emph{deze} steekproefwaarden een schatting te maken van $\beta_{0}$ en $\beta_{1}$.

\subsection{Schatting van $\beta_{0}$ en $\beta_{1}$ met de kleinste kwadraten methode}
De methodiek is bekend: parti\"ele afgeleiden naar $\beta_{0}$ en $\beta_{1}$ moeten gelijk aan nul zijn. Uit die twee vergelijkingen volgen twee getalwaarden $b_{0}$ en $b_{1}$ die berekend worden op basis van de steekproefgegevens. Hierbij worden wat tussenresultaten berekend. 
\begin{equation}
  \begin{split}
  E(X)&=\sum_{i=1}^{i=n}x_{i}\frac{1}{n} \\
  var(X)&=\frac{\sum_{i=1}^{i=n}(x_{i} - E(X))^2}{n-1}=\frac{SS_{xx}}{n-1} \\
  E(Y)&=\sum_{i=1}^{i=n}y_{i}\frac{1}{n} \\
  var(Y)&=\frac{\sum_{i=1}^{i=n}(y_{i} - E(Y))^2}{n-1}=\frac{SS_{yy}}{n-1} \\
  covar(X,Y)&=\frac{\sum_{i=1}^{i=n}(x_{i}-\bar(x))(y_{i}-\bar{y})}{n-1}=\frac{S_{xy}}{n-1}
  \end{split}
\end{equation}

<<echo=FALSE>>=
gem_x <- sum(geg$x)/n
ss_xx <- sum((geg$x-gem_x)^2)
var_x <- ss_xx/(n-1)
gem_y <- sum(geg$y)/n
ss_yy <- sum((geg$y - gem_y)^2)
var_y <- ss_yy/(n-1)
s_xy <- sum((geg$x-gem_x)*(geg$y-gem_y))
covar_xy <- s_xy/(n-1)
b1 <- s_xy/ss_xx
b0 <- gem_y - b1*gem_x
geg$diffx <- geg$x - gem_x
geg$diffx2 <- (geg$x - gem_x)^2
geg$diffy <- geg$y - gem_y
geg$diffy2 <- (geg$y - gem_y)^2
@

\begin{center}
\begin{tabular}{ r l }
 $\bar{x}=$ & \Sexpr{gem_x}  \\
 $SS_{xx}=$ & \Sexpr{ss_xx}  \\
 $var(x)=$ & \Sexpr{var_x} \\
 $\bar{y}=$ & \Sexpr{gem_y} \\
 $SS_{yy}=$ & \Sexpr{ss_yy} \\
 $var(y)=$ & \Sexpr{var_y} \\
 $S_{xy}=$ & \Sexpr{s_xy} \\
 $covar(x,y)=$ & \Sexpr{covar_xy} \\
 $b_{1}=\frac{\sum\limits_{i=1}^{i=n} (x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum\limits_{i=1}^{i=n}(x_{i}-\bar{x})^{2}}=\frac{S_{xy}}{SS_{xx}}=$ & \Sexpr{b1} \\
 $b_{0}=\bar{y} - b_{1}\bar{x}=$ & \Sexpr{round(b0,1)}
\end{tabular}
\end{center}
\medskip
Met deze waarden van $b_{0}$ en $b_{1}$ kunnen de berekende waarden en de fouten bepaald worden. De onderste rij geeft de som van alle kolommen.

\begin{equation}
  \begin{split}
  \hat{y}_{i} &= b_{0}+b_{1}x_{i} \\
  u_{i}&= y_{i} - \hat{y}_{i}
  \end{split}
\end{equation}

<<echo=FALSE>>=
geg$yhat <- round(b0 + b1*geg$x, 2)
geg$u <- geg$y - geg$yhat
geg$ux <- geg$x*geg$u
tabel <- geg
som <- round(colSums(geg),0)
tabel <- rbind(tabel,som)
@

<<>>=
tabel
@
\medskip
Wanneer we de parameters $b_{0}$ en $b_{1}$ bepalen met de kleinste kwadraten methode dan heeft de berekende rechte volgende eigenschappen:
\begin{itemize}
  \item de rechte gaat door het zwaartepunt $(\bar{x},\bar{y})$. Dit volgt uit de vergelijking voor $b_{0}$ die we kunnen herschrijven als $\bar{y}=b_{0}+b_{1}\bar{x}$.
  \item de som van de afwijkingen tussen de gemeten waarden $y_{i}$ en de berekende waarden $\hat{y}_{i}$ (de fouttermen $u_{i}$) is nul. Dat volgt uit
  \begin{equation}
  \sum\limits_{i=1}^{i=n}u_{i}=\sum\limits_{i=1}^{i=n}(y_{i}-b_{0}-b_{1}x_{i})=n\bar{y}-nb_{0}-nb_{1}\bar{x}=n(\bar{y}-b_{0}-b_{1}\bar{x})=0
  \end{equation}
  \item de afwijkingen $u_{i}$ en de onafhankelijke waarden $x_{i}$ zijn ortogonaal. Omslachtig bewijs\sidenote{''Kwantitatieve beleidsmethoden: enkelvoudige en meervoudige regressie", Peter Goos, 2014, pp. 13}
\end{itemize}

\subsection{Eigenschappen van de kleinste kwadratenschatters}

\newthought{Na het uitvoeren van een nieuw experiment} krijg je andere waarden voor $y_{i}$ (niet voor $x_{i}$!). Daaruit volgt dat $SS_{xx}$ hetzelfde blijft, maar $SS_{yy}$ en $S_{xy}$ veranderen. Daardoor verandert $b_{1}=\frac{S_{xy}}{SS_{xx}}$ en dus ook $b_{0}=\bar{y}-b_{1}\bar{x}$. Het toeval dat de fouten U genereert, zal dus ook de kleinste kwadratenschatters be\"ivloeden. Dit betekent dat $b_{0}$ en $b_{1}$ manifestaties zijn van kansvariabelen $B_{0}$ en $B_{1}$.

We kunnen bewijzen\sidenote{Goos pp. 14 e.v.} dat $b_{0}$ en $b_{1}$ onvertekende schatters zijn van de Platonische $\beta_{0}$ en $\beta_{1}$:
\begin{equation}
  \begin{split}
  E(B_{0})&=\beta_{0} \\
  E(B_{1})&=\beta_{1}
  \end{split}
\end{equation}

We kunnen ook de variantie (en dus de standaardafwijking) van $B_{0}$ en $B_{1}$ berekenen, mits wat bijkomende voorwaarden. De nieuwe veronderstellingen zijn:
\begin{itemize}
  \item \emph{homoscedasticiteit}: $var(U_{i})=\sigma^{2}=constant$\sidenote{Veronderstelling 3: de variantie van alle fouttermen is gelijk}
  \item \emph{onafhankelijkheid van de fouttermen}: $cov(U_{i},U_{j})=0$ voor $i \neq j$\sidenote{Veronderstelling 4}
\end{itemize}

In dat geval is
\begin{equation}
  \begin{split}
  var(B_{0}) &= \frac{\sigma^{2}}{SS_{xx}}\frac{\sum\limits_{i=1}^{i=n}x_{i}^{2}}{n} \\
  var(B_{1}) &= \frac{\sigma^{2}}{SS_{xx}} \\
  cov(B_{0}, B_{1}) &= -\frac{\sigma^{2}}{SS_{xx}}\bar{x}
  \end{split}
\end{equation}

We zien overal de breuk $\frac{\sigma^{2}}{SS_{xx}}$ opduiken, met in de teller de \emph{onbekende} variantie van de foutterm $\sigma^{2}$ en in de noemer de \emph{constante} $SS_{xx}$.

De \emph{Gauss-Markov stelling} toont aan dat de schatters voor $\beta_{0}$ $\beta_{1}$ die we bekomen via de kleinste kwadratenmethod niet alleen onvertekend zijn, maar bovendien de kleinste variantie hebben van alle onvertekende schatters die lineair zijn in de responsen $Y_{i}$.

\subsection{Een schatter voor $\sigma^{2}$}

De varianties van $B_{0}$ en $B_{1}$ zijn afhankelijk van de (constant veronderstelde) variantie van de foutterm. Aangezien $B_{0}$, $B_{1}$ en $Y_{i}$ kansvariabelen zijn, zal $\hat{U}_{i}=Y_{i}-B_{0}-B_{1}x_{i}$ ook een kansveranderlijke zijn en dus ook de som van de kwadraten ervan (SSE= Sum of Squared Errors):
\begin{equation}
SSE=\sum\limits_{i=1}^{i=n}\hat{U_{i}}^{2}=\sum\limits_{i=1}^{i=n}(Y_{i}-B_{0}-B_{1}x_{i})^{2}
\end{equation}

Het aantal vrijheidsgraden van SSE is (n-2) omdat twee vrijheidsgraden verloren gaan bij de schatting van $B_{0}$ en $B_{1}$. Men kan aantonen dat
\begin{equation}
E(\frac{SSE}{n-2})=E(MSE)=\sigma^{2}
\end{equation}

Voor \'e\'en bepaald experiment krijgt de kansveranderlijke SSE een concrete waarde nl;
\begin{equation}
sse=\sum\limits_{i=1}^{i=n}\hat{u_{i}}^{2}=\sum\limits_{i=1}^{i=n}(y_{i}-b_{0}-b_{1}x_{i})^{2}
\end{equation}

Een schatter voor de onbekende $\sigma^{2}$ is dus
\begin{equation}
s^{2}=\frac{sse}{n-2}=\frac{\sum\limits_{i=1}^{i=n}(y_{i}-b_{0}-b_{1}x_{i})^{2}}{n-2}=\frac{u_{i}^{2}}{n-2}
\end{equation}

<<echo=FALSE>>=
sse <- sum(geg$u^2)
s_sq <- sse/(n-2)
s <- sqrt(s_sq)
@

In dit voorbeeld is $s^{2}=$\Sexpr{round(s_sq,2)}. De schatters voor de variantie van $B_{0}$, $B_{1}$ en hun covariantie worden gegeven door:
\begin{equation}
  \begin{split}
  s^{2}_{B_{0}} &= \frac{s^{2}}{SS_{xx}}\frac{\sum\limits_{i=1}^{i=n}x_{i}^{2}}{n} = \frac{sse^{2}}{(n-2)SS_{xx}}\frac{\sum\limits_{i=1}^{i=n}x_{i}^{2}}{n} \\
  s^{2}_{B_{1}} &= \frac{s^{2}}{SS_{xx}} =  \frac{sse^{2}}{(n-2)SS_{xx}}\\
  s_{B_{0}, B_{1}} &= -\frac{s^{2}}{SS_{xx}}\bar{x} = -\frac{sse^{2}}{(n-2)SS_{xx}}\bar{x}
  \end{split}
\end{equation}

<<echo=FALSE>>=
s_b0 <- (s/sqrt(ss_xx))*(sqrt(sum(geg$x^2))/n)
s_b1 <- (s/sqrt(ss_xx))
@

De schatting voor de standaardafwijking van $b_{0}$ is \Sexpr{round(s_b0,2)} en van $b_{1}$ is \Sexpr{round(s_b1,2)}.

\subsection{Betrouwbaarheidsintervallen voor $\beta_{0}$ en $\beta_{1}$}
Als we veronderstellen\sidenote{Veronderstelling 5: de kansvariabelen $U_{i}$ zijn normaal verdeeld}, dan volgt daaruit dat ook de kansvariabelen $Y_{i}$ en dus ook $Y$ normaal verdeeld zijn (de som van een constante $E(Y_{i})$ en een normaal verdeelde kansvariabele). Aangezien de kleinste kwadratenschatters $B_{0}$ en $B_{1}$ lineaire combinaties zijn van de waarnemingen $Y_{i}$ zullen die ook normaal verdeeld zijn. We kennen hun gemiddelde waarde en hun variantie en kunnen dus direct schrijven dat:
\begin{equation}
  \begin{split}
  B_{0} &\sim N(\beta_{0}, \frac{\sigma}{\sqrt{SS_{xx}}}\sqrt{\frac{\sum\limits_{i=1}^{i=n}x_{i}^{2}}{n})} \\
  B_{1} &\sim N(\beta_{1}, \frac{\sigma}{\sqrt{SS_{xx}}})
  \end{split}
\end{equation}

We kennen de juiste waarde van $\sigma$ niet, maar wel de schatter ervan $s$. Dan weten we dat:
\begin{equation}
  \begin{split}
  \frac{B_{0} - \beta_{0}}{\frac{s}{\sqrt{SS_{xx}}}\sqrt{\frac{\sum\limits_{i=1}^{i=n}x_{i}^{2}}{n})}}  &\sim T_{(n-2)}  \\
  \frac{B_{1} - \beta_{1}}{\frac{s}{\sqrt{SS_{xx}}}} &\sim T_{n-2}
  \end{split}
\end{equation}

Hiermee kunnen we intervallen bepalen voor $\beta_{0}$ en $\beta_{1}$ met een betrouwbaarheid $\alpha$:
\begin{equation}
  \begin{split}
  t_{0}&=F_{T}^{-1}(1-\frac{\alpha}{2}, n-2) \\
  b_{0} - t_{0}\frac{s}{\sqrt{SS_{xx}}}\sqrt{\frac{\sum\limits_{i=1}^{i=n}x_{i}^{2}}{n})} &< \beta_{0} < b_{0} + t_{0}\frac{s}{\sqrt{SS_{xx}}}\sqrt{\frac{\sum\limits_{i=1}^{i=n}x_{i}^{2}}{n})} \\
  b_{1}-t_{0}\frac{s}{\sqrt{SS_{xx}}} &< \beta_{1} <  b_{1}+t_{0}\frac{s}{\sqrt{SS_{xx}}}
  \end{split}
\end{equation}

In R the inverse cumulatieve kansfunctie is $t_{0}=qt(1-\frac{\alpha}{2}, n-2)$. In dit voorbeeld zijn de 95\%-betrouwbaarheidsintervallen:
<<echo=FALSE>>=
betrouwbaarheid <- 0.95
alpha <- 1 - betrouwbaarheid
t0 <- qt(1 - alpha/2, n-2)
ogbeta0 <- b0 - t0*s_b0
bgbeta0 <- b0 + t0*s_b0
ogbeta1 <- b1 - t0*s_b1
bgbeta1 <- b1 + t0*s_b1
@
\begin{equation}
  \begin{split}
  \Sexpr{round(ogbeta0,2)} &< \beta_{0} < \Sexpr{round(bgbeta0,2)} \\
  \Sexpr{round(ogbeta1,2)} &< \beta_{1} < \Sexpr{round(bgbeta1,2)}
  \end{split}
\end{equation}

We kunnen ook de nulhypothese $H_{0}: \beta_{1}=0$ testen. De alternatieve hypothese is $H_{a}: \beta_{1} \neq 0$. Het is een tweezijdige toets. 

De toetsveranderlijke is $\phi^{*}=\frac{(b_{1}-\beta_{1})}{s_{b_{1}}}$

Deze is $T_{(n-2)}$-verdeeld. De p-waarde is dan gelijk aan $p=2*(1-F_{T}(|\phi^{*}|,(n-2)))=$\Sexpr{round(2*(1-pt(abs(b1/s_b1),n-2)),3)}.

\subsection{Kwaliteit van het model}
\newthought{De determinatieco\"effici\"ent}
\medskip
Opsplitsen van $SSTO$ in een som van $SSR$ en $SSE$. $SSR$ is de variantie die verklaard wordt door het model, $SSE$ het onverklaarde deel. De determinatieco\"effici\"ent is de verhouding van $SSR$ t.o.v. $SSTO$ en ligt tussen 0 (geen verband) en 1 (perfect lineair verband). De formules:
\begin{equation}
  \begin{split}
  SSTO &= \sum\limits_{i=1}^{i=n}(y_{i}-\bar{y})^{2} \\
  SSR &= \sum\limits_{i=1}^{i=n}(\hat{y}_{i}-\bar{y})^{2} \\
  SSE &= \sum\limits_{i=1}^{i=n}(\hat{y}_{i}-y_{i})^{2} \\
  SSTO &= SSR + SSE \\
  R^{2} &= \frac{SSR}{SSTO}=1-\frac{SSE}{SSTO} \\
  R^{2} &= b_{1}^{2}\frac{SS_{xx}}{SS_{yy}}
  \end{split}
\end{equation}

<<echo=FALSE>>=
R_sq <- (b1^2)*ss_xx/ss_yy
@

In dit voorbeeld is $R^{2}=$\Sexpr{round(R_sq,4)}.

Al het rekenwerk en de tussenresultaten zijn beschikbaar in een lijst die geproduceerd wordt wanneer we R een lineair model laten opmaken:

<<>>=
lineair_model <- lm(y ~ x, data=geg)
summary(lineair_model)
names(lineair_model)
@

\medskip
Tussenresultaten, zoals $SS_{xx}$ of $S_{xy}$ zijn niet te vinden.

\newthought{Je kan de significantie} van het lineair model toetsen door een Analysis Of Variance (anova) te doen van het lineair model. Daarin krijg je een overzicht van de vrijheidsgraden, de sums of squares ($SSR$ en $SSE$) en de daaruit afgeleide F-waarde en de p-waarde.

<<>>=
anova(lineair_model)
@

\subsection{Voorspellen gebruik makend van het lineair model}
\newthought{Voorspellen van $E(Y_{h})$}

We nemen een waarde van de onafhankelijk variabele $x=x_{h}$ en berekenen hiermee de waarde $\hat{y}_{h}=b_{0}+b_{1}x_{h}$. Het model doet een voorspelling voor $E(Y_{h})$, niet voor $y_{h}$ (dat bekijken we in volgende paragraaf). Omdat $B_{0}$ en $B_{1}$ kansvariabelen zijn (normaal verdeeld) zal $Y_{h}$ ook normaal verdeeld zijn met:\sidenote{Goos, pp. 28}
\begin{equation}
  \begin{split}
  E(Y_{h})&=E(B_{0}+B_{1}x_{h})=\beta_{0}+\beta_{1}x_{h} \\
  var(Y_{h})&= \sigma^{2}\left( \frac{1}{n} + \frac{(x_{h}-\bar{x})^{2}}{SS_{xx}} \right)
  \end{split}
\end{equation}

Dat betekent dat de variabele $\frac{(Y_{h}-E(Y_{h}))}{\sqrt{\sigma^{2}\left( \frac{1}{n} + \frac{(x_{h}-\bar{x})^{2}}{SS_{xx}}\right)}}$ T(n-2) verdeeld is, waarmee we een betrouwbaarheidsinterval kunnen berekenen voor $E(Y_{h})$:

\begin{equation}
  \begin{split}
  t_{0}&=F_{T}^{-1}(1-\frac{\alpha}{2}, n-2) \\
  \hat{Y}_{h}-t_{0}\sqrt{s^{2}\left( \frac{1}{n} + \frac{(x_{h}-\bar{x})^{2}}{SS_{xx}}\right)} &< E(Y_{h}) < \hat{Y}_{h}+t_{0}\sqrt{s^{2}\left( \frac{1}{n} + \frac{(x_{h}-\bar{x})^{2}}{SS_{xx}}\right)}
  \end{split}
\end{equation}

<<echo=FALSE>>=
x_h <- c(1:5)
betrouwbaarheid <- 0.95
alpha <- 1 - betrouwbaarheid
t0 <- qt(1 - alpha/2, n-2)
s_Eyh <- s*sqrt((1/n + ((x_h - gem_x)^2)/ss_xx))
Eyh <- b0 + b1*x_h
ogEyh <- Eyh - t0*s_Eyh
bgEyh <- Eyh + t0*s_Eyh
grenzen_Eyh <- data.frame(xh=x_h, fit= Eyh, ondergrens=ogEyh, bovengrens=bgEyh)
@

<<>>=
grenzen_Eyh
@

In R gebruiken we de \emph{predict} functie: je start met de $x_{h}$-waarden waarvoor je een voorspelling van $E(y_{h})$ wil. Die moeten in een data-frame geplaatst met als kolomnaam dezelfde naam als diegene die gebruikt werd voor de onafhankelijke veranderlijke bij de opstelling van het lineair model (in dit gevel $x$). Je kan dan de voorspelde waarde(n) en de $(1-\alpha)$ onder- en bovengrens. Bijkomende info is mogelijk bv. de standard error.

<<>>=
pred_xh <- data.frame(x=x_h)
predict(lineair_model, pred_xh, interval= "confidence", level=0.95, se.fit=TRUE)
@

\newthought{Wanneer je de waarde van een nieuw experiment} $y_{h}$ wil voorspellen dan volgt uit:
\begin{equation}
Y_{h}=E(Y_{h}) + U_{h} = \hat{Y}_{h} + U_{h}
\end{equation}

dat $Y_{h}$ ook normaal zal verdeeld zijn omdat $U \sim N(0, \sigma)$ verdeeld is. De gemiddelde waarde van $Y_{h}$ zal dus ook gelijk zijn aan $\hat{Y}_{h}$ en de variantie van $Y_{h}$ zal gelijk zijn aan de variantie van $E(Y_{h})$ plus de variantie van $U_{h}=\sigma^{2}$. Het betrouwbaarheidsinterval voor $Y_{h}$ wordt dus wat groter:
\begin{equation}
  \begin{split}
  t_{0}&=F_{T}^{-1}(1-\frac{\alpha}{2}, n-2) \\
  \hat{Y}_{h}-t_{0}\sqrt{s^{2}\left(1 + \frac{1}{n} + \frac{(x_{h}-\bar{x})^{2}}{SS_{xx}}\right)} &< Y_{h} < \hat{Y}_{h}+t_{0}\sqrt{s^{2}\left(1 + \frac{1}{n} + \frac{(x_{h}-\bar{x})^{2}}{SS_{xx}}\right)}
  \end{split}
\end{equation}

<<echo=FALSE>>=
s_yh <- s*sqrt((1 + 1/n + ((x_h - gem_x)^2)/ss_xx))
ogyh <- Eyh - t0*s_yh
bgyh <- Eyh + t0*s_yh
grenzen_yh <- data.frame(xh=x_h, fit=Eyh, ondergrens=ogyh, bovengrens=bgyh)
@

<<>>=
grenzen_yh
@

In R gebruiken we opnieuw de \emph{predict} functie met als keuze "predict" voor de parameter \emph{interval}:

<<>>=
predict(lineair_model, pred_xh, interval= "predict", level=0.95)
@

\subsection{Gebruik van package ''broom"}
Dit package omvat 3 functies: ''tidy", ''augment" en ''glance". Hierdoor wordt de informatie die in het model object zit (meestal een lijst) omgezet naar een data.frame zodat je daar veel gemakkelijker onderdelen uit kan oproepen.

<<>>=
broom::tidy(lineair_model)
broom::augment(lineair_model)
broom::glance(lineair_model)
@

\subsection{Controle van de veronderstellingen}

\newthought{Veronderstelling 1}: $E(U_{i})=0$

\newthought{Veronderstelling 2}: $E(Y)=\beta_{0}+\beta_{1}x$
Behalve indien je theo-retische gronden hebt om dit functievoor-schrift voor te stellen (en dan spreken we over een regressie en niet een corelatie), zie ik geen mogelijkheid om dit te controleren. Ik vermoed dat deze aanname meestal puur visueel gebeurt.

\newthought{Veronderstelling 3}: $var(U_{i})=\sigma^{2}=constant$

\newthought{Veronderstelling 4}: $cov(U_{i},U_{j})=0$ voor $i \neq j$

\newthought{Veronderstelling 5}: $U \sim N(0,\sigma)$
\end{document}